The problems increases with more data in big data world. The main problem is data processing has more cost with data memory has more disk usage. However there are many solutions but one of them is The Apache Spark. In this article;

* What is Apache Spark?
* How to optimize Apache Spark?
* What is Delta Lake?
* Apache Spark Project. <br>
This project is versatile. Project follows these steps:

1. Run Apache Hadoop, Apache Hive
2. Execute Apache Spark by PySpark with Delta Lake configuration
3. Read / Process Data
4. Optimize Apache Spark. Save data on Apache Hive, PostgreSQL and Delta Lake
5. Import saved data from Apache Hive, PostgreSQL and Delta Lake.
6. Stop Apache Spark <br>
For more details: https://medium.com/@TalhaNebiKumru/data-processing-optimization-transformation-to-apache-hive-postgresql-and-delta-lake-on-apache-941b564ddc35
